# Importing Libraries

# for data manipulation
import numpy as np
import pandas as pd

# for data visualizations
import matplotlib.pyplot as plt
import seaborn as sns

# for data preprocessing
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# for model training 

from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier,plot_tree
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import BernoulliNB

# for model evaluation 

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn.metrics import roc_curve,roc_auc_score

# miscellaneous

import warnings
warnings.filterwarnings('ignore')

# Data Gathering

df = pd.read_csv(r"C:\Users\Dell\Downloads\Loan_Data.csv")
print(df)

# Exploratory Data Analysis

print(df.shape)                                # gives number of rows and colomns
print(df.columns)                              # give all the column names
print(df["Loan_Amount_Term"].unique())
print(df["Property_Area"].unique())
print(df["Credit_History"].unique())

print(df.describe())                           # give all the statistical information
print(df.info())                               # gives all the basic information

print(df.isna().sum())                         # ckecks if the column have missing values

# checking if any columns have outliers
print(sns.boxenplot(df["ApplicantIncome"]))
print(sns.boxenplot(df["CoapplicantIncome"]))
print(sns.boxenplot(df["LoanAmount"]))
print(sns.boxenplot(df["Loan_Amount_Term"]))
print(sns.boxenplot(df["Credit_History"]))

# Check outliers

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3-Q1

LowerTail = Q1-1.5*IQR
UpperTail = Q3+1.5*IQR

show_outliers = (df<LowerTail)|(df>UpperTail)
outlier_count = show_outliers.sum()
print(outlier_count)

# finding correlation
print(sns.heatmap(df.corr(),annot=True))

# Feature Engineering

# droping unnecessary collumns

df = df.drop(["Loan_ID"],axis=1)
print(df)

# Handling Outliers

def remove_outliers(column):
    q1 = column.quantile(0.25)
    q3 = column.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    return column[(column >= lower_bound) & (column <= upper_bound)]

new_df = df.apply(lambda col: remove_outliers(col) if np.issubdtype(col.dtype, np.number) else col)

print(new_df)

show_outliers = (new_df<LowerTail)|(new_df>UpperTail)
outlier_count = show_outliers.sum()
print(outlier_count)

# Handle missing values

new_df.isna().sum()                          # ckecks if the column have missing values

# Gender columns
print(new_df["Gender"].value_counts())
new_df["Gender"] = new_df["Gender"].fillna("Male")

# Married column
print(new_df["Married"].value_counts())
new_df["Married"] = new_df["Married"].fillna("Yes")

# Dependents column
print(new_df["Dependents"].value_counts())
new_df["Dependents"] = new_df["Dependents"].fillna("0")

# Self_Employed column
print(new_df["Self_Employed"].value_counts())
new_df["Self_Employed"] = new_df["Self_Employed"].fillna("No")

# ApplicantIncome column
print(new_df["ApplicantIncome"].value_counts())
print(new_df.describe())
new_df["ApplicantIncome"] = new_df["ApplicantIncome"].fillna(4124.723404)           # mean value taken by new_df.describe()

# CoapplicantIncome
print(new_df["CoapplicantIncome"].value_counts())
new_df["CoapplicantIncome"] = new_df["CoapplicantIncome"].fillna(0)

# LoanAmount
print(new_df["LoanAmount"].value_counts())
new_df["LoanAmount"] = new_df["LoanAmount"].fillna(129.000000)                      # mean value taken by new_df.describe()

# Loan_Amount_Term
print(new_df["Loan_Amount_Term"].value_counts())         
print(df["Loan_Amount_Term"].value_counts())                                       # ckecking value count before outlier handling

new_df["Loan_Amount_Term"] = df["Loan_Amount_Term"].fillna(360.0)                  # used df instead of new_df so data will get different values

# Credit_History
df["Credit_History"].value_counts()
new_df["Credit_History"] = df["Credit_History"].fillna(1.0)                        # used df instead of new_df so data will get different values
new_df["Credit_History"].replace({1.0:"1",0.0:"0"},inplace=True)                   # Used replace fuction to conver float to text so we can apply label encoding

# checking data after imputation

print(new_df.isna().sum())
print(new_df.info())

# Encoding

label_encoder = LabelEncoder()

new_df["Gender"] = label_encoder.fit_transform(new_df["Gender"])
new_df["Married"] = label_encoder.fit_transform(new_df["Married"])
new_df["Dependents"] = label_encoder.fit_transform(new_df["Dependents"])
new_df["Education"] = label_encoder.fit_transform(new_df["Education"])
new_df["Self_Employed"] = label_encoder.fit_transform(new_df["Self_Employed"])
new_df["Credit_History"] = label_encoder.fit_transform(new_df["Credit_History"])
new_df["Loan_Status"] = label_encoder.fit_transform(new_df["Loan_Status"])

new_df = pd.get_dummies(new_df,columns = ["Property_Area"])                         # OneHotEncoding

print(new_df.info())                                                                # to check if the text data is converted into numerical

# Scaling

#std_scaler = StandardScaler()
#std_scaler.fit_transform(new_df)

# Normalization

MinMax = MinMaxScaler()
MinMax.fit_transform(new_df)

# Feature Selection

# Checking Linearity (Corelation between Dependant and Independant Columns) After Feature Engineering

print(new_df.corr())
print(sns.heatmap(new_df.corr(),annot=True))

# Model Training

# spliting the training and testing data

x = new_df.drop(["Loan_Status"],axis=1)
y = new_df["Loan_Status"]

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=10)

# Logistic Regression

log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)

y_train_pred = log_reg.predict(x_train)
y_test_pred = log_reg.predict(x_test)

# Model Evaluation For Logistic Regression

# for training

accuracy_Train = accuracy_score(y_train,y_train_pred)
print(f"accuracy score for training = {accuracy_Train}")
print("*"*60)
conf_matrix_Train = confusion_matrix(y_train,y_train_pred)
print("confusion matrix for training")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Train, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Train = classification_report(y_train,y_train_pred)
print("classisfication report for training")
print(class_report_Train)

# for testing

accuracy_Test = accuracy_score(y_test,y_test_pred)
print(f"accuracy score for testing = {accuracy_Test}")
print("*"*60)
conf_matrix_Test = confusion_matrix(y_test,y_test_pred)
print("confusion matrix for testing")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Test, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Test = classification_report(y_test,y_test_pred)
print("classisfication report for testing")
print(class_report_Test)

result = []
 
def Final_result(model_name, acc_score_training, acc_score_testing):
     if model_name not in result:
            result.append([model_name, (acc_score_training*100),(acc_score_testing*100)])
            final_df = pd.DataFrame(result, columns=['Model Name', 'accurary score for training', 'accuracy score for testing'])
            return final_df

Final_result("Logistic Regression Training", accuracy_Train, accuracy_Test)

# ROC_Curve

prob = log_reg.predict_proba(x_train)                                        #actual probabilities

fpr,tpr,th = roc_curve(y_train,prob[:,:1])

plt.plot(tpr,fpr)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.plot(y_train,y_train)
plt.title("Receiver Operating Characteristic (ROC) Curve")

# ROC_AUC_Score                                                 # Area Under Curve (AUC) Score

ROC_AUC_Score = roc_auc_score(y_train,prob[:,:1])

print(f"Area Under Curve (AUC) Score = {ROC_AUC_Score}")

# Decision Tree Classifier

dt_clf = DecisionTreeClassifier()
dt_clf.fit(x_train,y_train)

y_train_pred_dt = dt_clf.predict(x_train)
y_test_pred_dt = dt_clf.predict(x_test)

# Model Evaluation For Decision Tree Classifier

# for training

accuracy_Train_dt = accuracy_score(y_train,y_train_pred_dt)
print(f"accuracy score for training = {accuracy_Train_dt}")
print("*"*60)
conf_matrix_Train_dt = confusion_matrix(y_train,y_train_pred_dt)
print("confusion matrix for training")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Train_dt, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Train_dt = classification_report(y_train,y_train_pred_dt)
print("classisfication report for training")
print(class_report_Train_dt)

# for testing

accuracy_Test_dt = accuracy_score(y_test,y_test_pred_dt)
print(f"accuracy score for testing = {accuracy_Test_dt}")
print("*"*60)
conf_matrix_Test_dt = confusion_matrix(y_test,y_test_pred_dt)
print("confusion matrix for testing")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Test_dt, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Test_dt = classification_report(y_test,y_test_pred_dt)
print("classisfication report for testing")
print(class_report_Test_dt)

# Plot Decision Tree

plt.figure(figsize=(200,160))
plot_tree(dt_clf, feature_names=x.columns,class_names=['0','1'],filled=True)
plt.show()

# Hyperparameter Tuning And Cross Validation

hyperparameters = {"criterion" : ['gini','entropy'],
                   "max_depth" : np.arange(3,6),
                   "min_samples_split" : np.arange(5,16),
                   "min_samples_leaf": np.arange(5,11)}

# cross validation

GSCV_dt_Model = GridSearchCV(dt_clf,hyperparameters,cv=5)
GSCV_dt_Model.fit(x_train,y_train)
GSCV_dt_Model.best_estimator_

Tuned_dt_model = DecisionTreeClassifier(max_depth=3, min_samples_leaf=9, min_samples_split=5)
Tuned_dt_model.fit(x_train,y_train)

y_train_pred_dt_tuned = Tuned_dt_model.predict(x_train)
y_test_pred_dt_tuned = Tuned_dt_model.predict(x_test)

# Model Evaluation After Tuning And Cross Validation

# for training

accuracy_Train_dt_tuned = accuracy_score(y_train,y_train_pred_dt_tuned)
print(f"accuracy score for training = {accuracy_Train_dt_tuned}")
print("*"*60)
conf_matrix_Train_dt_tuned = confusion_matrix(y_train,y_train_pred_dt_tuned)
print("confusion matrix for training")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Train_dt_tuned, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Train_dt_tuned = classification_report(y_train,y_train_pred_dt_tuned)
print("classisfication report for training")
print(class_report_Train_dt_tuned)

# For Testing

accuracy_Test_dt_tuned = accuracy_score(y_test,y_test_pred_dt_tuned)
print(f"accuracy score for testing = {accuracy_Test_dt_tuned}")
print("*"*60)
conf_matrix_Test_dt_tuned = confusion_matrix(y_test,y_test_pred_dt_tuned)
print("confusion matrix for testing")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Test_dt_tuned, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Test_dt_tuned = classification_report(y_test,y_test_pred_dt_tuned)
print("classisfication report for testing")
print(class_report_Test_dt_tuned)

Final_result("Decision Tree Classifier",accuracy_Train_dt_tuned,accuracy_Test_dt_tuned)

# Ensemble Learning Techniques

# Random Forest Classifier

rf_clf = RandomForestClassifier()
rf_clf.fit(x_train,y_train)

y_train_pred_rf = rf_clf.predict(x_train)
y_test_pred_rf = rf_clf.predict(x_test)

# Model Evaluation For Random Forest Classifier

# for training

accuracy_Train_rf = accuracy_score(y_train,y_train_pred_rf)
print(f"accuracy score for training = {accuracy_Train_rf}")
print("*"*60)
conf_matrix_Train_rf = confusion_matrix(y_train,y_train_pred_rf)
print("confusion matrix for training")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Train_rf, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Train_rf = classification_report(y_train,y_train_pred_rf)
print("classisfication report for training")
print(class_report_Train_rf)

# for testing

accuracy_Test_rf = accuracy_score(y_test,y_test_pred_rf)
print(f"accuracy score for testing = {accuracy_Test_rf}")
print("*"*60)
conf_matrix_Test_rf = confusion_matrix(y_test,y_test_pred_rf)
print("confusion matrix for testing")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Test_rf, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Test_rf = classification_report(y_test,y_test_pred_rf)
print("classisfication report for testing")
print(class_report_Test_rf)

# Hyperparameter Tuning And Cross Validation

Hyperparameters_rf = {"n_estimators":np.arange(10,200),
                      "criterion": ['gini','entropy'],
                      "max_depth" : np.arange(5,15),
                      "min_samples_split":np.arange(5,20),
                      "min_samples_leaf":np.arange(4,15),
                      "max_features":['auto']}

RSCV_rf_clf = RandomizedSearchCV(rf_clf,Hyperparameters_rf,cv=6)

RSCV_rf_clf.fit(x_train,y_train)

RSCV_rf_clf.best_estimator_

Tuned_rf_model = RandomForestClassifier(max_depth=11, max_features='auto',
                       min_samples_leaf=5, min_samples_split=5,
                       n_estimators=28)

Tuned_rf_model.fit(x_train,y_train)

y_train_pred_rf_tuned = Tuned_rf_model.predict(x_train)
y_test_pred_rf_tuned = Tuned_rf_model.predict(x_test)

# Model Evaluation After Hyperparameter Tuning And Cross Validation

# for training

accuracy_Train_rf_tuned = accuracy_score(y_train,y_train_pred_rf_tuned)
print(f"accuracy score for training = {accuracy_Train_rf_tuned}")
print("*"*60)
conf_matrix_Train_rf_tuned = confusion_matrix(y_train,y_train_pred_rf_tuned)
print("confusion matrix for training")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Train_rf_tuned, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Train_rf_tuned = classification_report(y_train,y_train_pred_rf_tuned)
print("classisfication report for training")
print(class_report_Train_rf_tuned)

# For testing

accuracy_Test_rf_tuned = accuracy_score(y_test,y_test_pred_rf_tuned)
print(f"accuracy score for testing = {accuracy_Test_rf_tuned}")
print("*"*60)
conf_matrix_Test_rf_tuned = confusion_matrix(y_test,y_test_pred_rf_tuned)
print("confusion matrix for testing")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Test_rf_tuned, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Test_rf_tuned = classification_report(y_test,y_test_pred_rf_tuned)
print("classisfication report for testing")
print(class_report_Test_rf_tuned)

Final_result("Random Forest Classifier",accuracy_Train_rf_tuned,accuracy_Test_rf_tuned)

# Adaboost (Adaptive Boosting) Classifier

adb_clf = AdaBoostClassifier()

# Hyperparameter Tuning And Cross validation

hyp_adb = {"n_estimators":np.arange(10,200),
       "learning_rate" : np.array([0.01,0,0.1,1])}

rscv_adb_clf = RandomizedSearchCV(adb_clf, hyp_adb, cv = 6)

rscv_adb_clf.fit(x_train,y_train)

rscv_adb_clf.best_estimator_

tuned_adb_model = AdaBoostClassifier(learning_rate=0.01, n_estimators=82)

tuned_adb_model.fit(x_train,y_train)

y_train_pred_abd_tuned = tuned_adb_model.predict(x_train)
y_test_pred_adb_tuned = tuned_adb_model.predict(x_test)

# Model Evaluation For AdaBoost Classiefier

# for training 

accuracy_Train_adb_tuned = accuracy_score(y_train,y_train_pred_abd_tuned)
print(f"accuracy score for training = {accuracy_Train_adb_tuned}")
print("*"*60)
conf_matrix_Train_abd_tuned = confusion_matrix(y_train,y_train_pred_abd_tuned)
print("confusion matrix for training")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Train_abd_tuned, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Train_abd_tuned = classification_report(y_train,y_train_pred_abd_tuned)
print("classisfication report for training")
print(class_report_Train_abd_tuned)

# for testing

accuracy_Test_adb_tuned = accuracy_score(y_test,y_test_pred_adb_tuned)
print(f"accuracy score for testing = {accuracy_Test_adb_tuned}")
print("*"*60)
conf_matrix_Test_adb_tuned = confusion_matrix(y_test,y_test_pred_adb_tuned)
print("confusion matrix for testing")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Test_adb_tuned, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Test_adb_tuned = classification_report(y_test,y_test_pred_adb_tuned)
print("classisfication report for testing")
print(class_report_Test_adb_tuned)

Final_result("Adaboost Classifier",accuracy_Train_adb_tuned,accuracy_Test_adb_tuned)

# K-Nearest-Neighbours Classifier

knn = KNeighborsClassifier()

# Hyperparameter Tuning And Cross Validation

hyp_knn = {"n_neighbors":np.arange(2,30),"p":[1,2]}

RSCV_KNN = GridSearchCV(knn, hyp_knn, cv=5)

RSCV_KNN.fit(x_train,y_train)

RSCV_KNN.best_estimator_

tuned_knn = KNeighborsClassifier(n_neighbors=23, p=1)

tuned_knn.fit(x_train,y_train)

y_train_pred_knn_tuned = tuned_knn.predict(x_train)
y_test_pred_knn_tuned = tuned_knn.predict(x_test)

# Model Evaluation For KNN Classifier

# for training

accuracy_Train_knn_tuned = accuracy_score(y_train,y_train_pred_knn_tuned)
print(f"accuracy score for training = {accuracy_Train_knn_tuned}")
print("*"*60)
conf_matrix_Train_knn_tuned = confusion_matrix(y_train,y_train_pred_knn_tuned)
print("confusion matrix for training")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Train_knn_tuned, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Train_knn_tuned = classification_report(y_train,y_train_pred_knn_tuned)
print("classisfication report for training")
print(class_report_Train_knn_tuned)

# for testing

accuracy_Test_knn_tuned = accuracy_score(y_test,y_test_pred_knn_tuned)
print(f"accuracy score for testing = {accuracy_Test_knn_tuned}")
print("*"*60)
conf_matrix_Test_knn_tuned = confusion_matrix(y_test,y_test_pred_knn_tuned)
print("confusion matrix for testing")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Test_knn_tuned, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Test_knn_tuned = classification_report(y_test,y_test_pred_knn_tuned)
print("classisfication report for testing")
print(class_report_Test_knn_tuned)

Final_result("K-Nearest-Neighbours Classifier",accuracy_Train_knn_tuned,accuracy_Test_knn_tuned)

# Support Vector Machine

svc = SVC()

svc.fit(x_train,y_train)

y_train_pred_scv = svc.predict(x_train)
y_test_pred_scv = svc.predict(x_test)

# Model Evaluation For Support Vector Machine

# for training

accuracy_Train_scv = accuracy_score(y_train,y_train_pred_scv)
print(f"accuracy score for training = {accuracy_Train_scv}")
print("*"*60)
conf_matrix_Train_svc = confusion_matrix(y_train,y_train_pred_scv)
print("confusion matrix for training")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Train_svc, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Train_svc = classification_report(y_train,y_train_pred_scv)
print("classisfication report for training")
print(class_report_Train_svc)

# for testing

accuracy_Test_svc = accuracy_score(y_test,y_test_pred_scv)
print(f"accuracy score for testing = {accuracy_Test_svc}")
print("*"*60)
conf_matrix_Test_svc = confusion_matrix(y_test,y_test_pred_scv)
print("confusion matrix for testing")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Test_svc, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Test_svc = classification_report(y_test,y_test_pred_scv)
print("classisfication report for testing")
print(class_report_Test_svc)

Final_result("Support Vector Classifier",accuracy_Train_scv,accuracy_Test_svc)

# Naive Bayes Classifier (BernoulliNB for Binary Classification)

bnb_clf = BernoulliNB()

bnb_clf.fit(x_train,y_train)

y_train_pred_bnb = bnb_clf.predict(x_train)
y_test_pred_bnb = bnb_clf.predict(x_test)

# Model Evaluation For Naive Bayes Classifier

# for training

accuracy_Train_bnb = accuracy_score(y_train,y_train_pred_bnb)
print(f"accuracy score for training = {accuracy_Train_bnb}")
print("*"*60)
conf_matrix_Train_bnb = confusion_matrix(y_train,y_train_pred_bnb)
print("confusion matrix for training")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Train_bnb, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Train_bnb = classification_report(y_train,y_train_pred_bnb)
print("classisfication report for training")
print(class_report_Train_bnb)

# for testing

accuracy_Test_bnb = accuracy_score(y_test,y_test_pred_bnb)
print(f"accuracy score for testing = {accuracy_Test_bnb}")
print("*"*60)
conf_matrix_Test_bnb = confusion_matrix(y_test,y_test_pred_bnb)
print("confusion matrix for testing")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Test_bnb, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Test_bnb = classification_report(y_test,y_test_pred_bnb)
print("classisfication report for testing")
print(class_report_Test_bnb)

Final_result("Naive Bayes Classifier (BernoulliNB)",accuracy_Train_bnb,accuracy_Test_bnb)

# Model Evaluation Conclusion

final_evaluation = pd.DataFrame(result,columns=["Model Name","Accuracy Score For Training","Accuracy Score For Testing"])
print(final_evaluation)

# Conclusion

print('''In the Load Eligibility machine learning project, seven algorithms were tested to predict Loan Status, with the Random Forest Classifier emerging as the most accurate model. 
With hyperparameter tuning, the Random Forest model achieved 83% accuracy on the training data and 80% accuracy on the testing data. 
This underscores the effectiveness of machine learning techniques in accurately predicting Loan Status, with Random Forest demonstrating superior performance in this context.
''')