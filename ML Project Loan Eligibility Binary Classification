# Importing Libraries

# for data manipulation
import numpy as np
import pandas as pd

# for data visualizations
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import statsmodels.api as sm

# for data preprocessing
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# for model training 

from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier,plot_tree
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB,BernoulliNB

# for model evaluation 

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn.metrics import roc_curve,roc_auc_score

# for basic statistical operations

from statsmodels.stats.outliers_influence import variance_inflation_factor
from scipy import stats
from scipy.stats import shapiro

# miscellaneous
import pickle
import warnings
warnings.filterwarnings('ignore')

# Data Gathering

df = pd.read_csv(r"C:\Users\Dell\Downloads\Loan_Data.csv")
print(df)

# Exploratory Data Analysis

print(df.shape)                                # gives number of rows and colomns
print(df.columns)                              # give all the column names
print(df["Loan_Amount_Term"].unique())
print(df["Property_Area"].unique())
print(df["Credit_History"].unique())

print(df.describe())                           # give all the statistical information
print(df.info())                               # gives all the basic information

print(df.isna().sum())                         # ckecks if the column have missing values

# checking if any columns have outliers
print(sns.boxenplot(df["ApplicantIncome"]))
print(sns.boxenplot(df["CoapplicantIncome"]))
print(sns.boxenplot(df["LoanAmount"]))
print(sns.boxenplot(df["Loan_Amount_Term"]))
print(sns.boxenplot(df["Credit_History"]))

# Check outliers

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3-Q1

LowerTail = Q1-1.5*IQR
UpperTail = Q3+1.5*IQR

show_outliers = (df<LowerTail)|(df>UpperTail)
outlier_count = show_outliers.sum()
print(outlier_count)

# finding correlation
print(sns.heatmap(df.corr(),annot=True))

# Feature Engineering

# droping unnecessary collumns

df = df.drop(["Loan_ID"],axis=1)
print(df)

# Handling Outliers

def remove_outliers(column):
    q1 = column.quantile(0.25)
    q3 = column.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    return column[(column >= lower_bound) & (column <= upper_bound)]

new_df = df.apply(lambda col: remove_outliers(col) if np.issubdtype(col.dtype, np.number) else col)

print(new_df)

show_outliers = (new_df<LowerTail)|(new_df>UpperTail)
outlier_count = show_outliers.sum()
print(outlier_count)

# Handle missing values

new_df.isna().sum()                          # ckecks if the column have missing values

# Gender columns
print(new_df["Gender"].value_counts())
new_df["Gender"] = new_df["Gender"].fillna("Male")

# Married column
print(new_df["Married"].value_counts())
new_df["Married"] = new_df["Married"].fillna("Yes")

# Dependents column
print(new_df["Dependents"].value_counts())
new_df["Dependents"] = new_df["Dependents"].fillna("0")

# Self_Employed column
print(new_df["Self_Employed"].value_counts())
new_df["Self_Employed"] = new_df["Self_Employed"].fillna("No")

# ApplicantIncome column
print(new_df["ApplicantIncome"].value_counts())
print(new_df.describe())
new_df["ApplicantIncome"] = new_df["ApplicantIncome"].fillna(4124.723404)           # mean value taken by new_df.describe()

# CoapplicantIncome
print(new_df["CoapplicantIncome"].value_counts())
new_df["CoapplicantIncome"] = new_df["CoapplicantIncome"].fillna(0)

# LoanAmount
print(new_df["LoanAmount"].value_counts())
new_df["LoanAmount"] = new_df["LoanAmount"].fillna(129.000000)                      # mean value taken by new_df.describe()

# Loan_Amount_Term
print(new_df["Loan_Amount_Term"].value_counts())         
print(df["Loan_Amount_Term"].value_counts())                                       # ckecking value count before outlier handling

new_df["Loan_Amount_Term"] = df["Loan_Amount_Term"].fillna(360.0)                  # used df instead of new_df so data will get different values

# Credit_History
df["Credit_History"].value_counts()
new_df["Credit_History"] = df["Credit_History"].fillna(1.0)                        # used df instead of new_df so data will get different values
new_df["Credit_History"].replace({1.0:"1",0.0:"0"},inplace=True)                   # Used replace fuction to conver float to text so we can apply label encoding

# checking data after imputation

print(new_df.isna().sum())
print(new_df.info())

# Encoding

label_encoder = LabelEncoder()

new_df["Gender"] = label_encoder.fit_transform(new_df["Gender"])
new_df["Married"] = label_encoder.fit_transform(new_df["Married"])
new_df["Dependents"] = label_encoder.fit_transform(new_df["Dependents"])
new_df["Education"] = label_encoder.fit_transform(new_df["Education"])
new_df["Self_Employed"] = label_encoder.fit_transform(new_df["Self_Employed"])
new_df["Credit_History"] = label_encoder.fit_transform(new_df["Credit_History"])
new_df["Loan_Status"] = label_encoder.fit_transform(new_df["Loan_Status"])

new_df = pd.get_dummies(new_df,columns = ["Property_Area"])                         # OneHotEncoding

print(new_df.info())                                                                # to check if the text data is converted into numerical

# scaling

std_scaler = StandardScaler()
std_scaler.fit_transform(new_df)

# Feature Selection

# Checking Linearity (Corelation between Dependant and Independant Columns) After Feature Engineering

print(new_df.corr())
print(sns.heatmap(new_df.corr(),annot=True))

# Model Training

# spliting the training and testing data

x = new_df.drop(["Loan_Status"],axis=1)
y = new_df["Loan_Status"]

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=10)

# Logistic Regression

log_reg = LogisticRegression()
log_reg.fit(x_train,y_train)

y_train_pred = log_reg.predict(x_train)
y_test_pred = log_reg.predict(x_test)

# model evaluation for Logistic Regression

# for training

accuracy_Train = accuracy_score(y_train,y_train_pred)
print(f"accuracy score for training = {accuracy_Train}")
print("*"*60)
conf_matrix_Train = confusion_matrix(y_train,y_train_pred)
print("confusion matrix for training")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Train, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Train = classification_report(y_train,y_train_pred)
print("classisfication report for training")
print(class_report_Train)

# for testing

accuracy_Test = accuracy_score(y_test,y_test_pred)
print(f"accuracy score for testing = {accuracy_Test}")
print("*"*60)
conf_matrix_Test = confusion_matrix(y_test,y_test_pred)
print("confusion matrix for testing")
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix_Test, annot=True, fmt='.2f', cmap='coolwarm') 
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
print("*"*60)
class_report_Test = classification_report(y_test,y_test_pred)
print("classisfication report for testing")
print(class_report_Test)

# ROC_Curve and ROC_AUC_Score

prob = log_reg.predict_proba(x_train)                                        #actual probabilities

fpr,tpr,th = roc_curve(y_train,prob[:,:1])

plt.plot(tpr,fpr)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.plot(y_train,y_train)
plt.title("Receiver Operating Characteristic (ROC) Curve")

# ROC_AUC_Score                                                 # Area Under Curve (AUC) Score

ROC_AUC_Score = roc_auc_score(y_train,prob[:,:1])

print(f"Area Under Curve (AUC) Score = {ROC_AUC_Score}")

# Decision Tree Classifier

dt_clf = DecisionTreeClassifier()
dt_clf.fit(x_train,y_train)

y_train_pred_dt = dt_clf.predict(x_train)
y_test_pred_dt = dt_clf.predict(x_test)